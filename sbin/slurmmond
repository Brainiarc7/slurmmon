#!/usr/bin/env python

"""\
NAME
	slurmmond - gather data about SLURM behavior

SYNOPSIS
	slurmmond

DESCRIPTION
	This runs several child processes that loop reporting back to the parent various metrics.
	The master process sends these to ganglia by running gmetric.

OPTIONS
	--daemon
		Run as a daemon instead of in the foreground as a child of the calling process.

	-v, --verbose
		Write metrics to stdout, too (as gmetric commands).
	
	-d, --debug
		Log extra debugging information.
	
	-p, --pretend
		Don't actually send gmetrics.
	
	--log-to-stderr
		Log messages to stderr in addition to syslog.

	-h, --help
		Print this help.

REQUIREMENTS
	n/a

BUGS/TODO
	n/a

AUTHOR
	Copyright (c) 2013
	Harvard FAS Research Computing
	John A. Brunelle
"""

"""
architecture
	metrics for individual categories are run as parallel sub-processes (using multiprocessing)
	they yield metrics -- tuples in the form (NAME, VALUE, TYPE, UNITS, GROUP, TITLE, DESCRIPTION)

metrics (subject to change)
	#--jobcount

#	slurmmond_jobcount_total_pending.rrd (jobs)
#	slurmmond_jobcount_total_running.rrd (jobs)
#	slurmmond_jobcount_max_pending_one_user.rrd (jobs)
#	slurmmond_jobcount_max_running_one_user.rrd (jobs)


	#--- probejob

#	slurmmond_probejob_pendtime_general.rrd (seconds)
#	slurmmond_probejob_pendtime_interact.rrd (seconds)


	#--- sdiag

#	slurmmond_sdiag_server_thread_count.rrd (threads)
#	slurmmond_sdiag_agent_queue_size.rrd (agents)

#	slurmmond_sdiag_jobs_submitted.rrd (jobs)
#	slurmmond_sdiag_jobs_started.rrd (jobs)
#	slurmmond_sdiag_jobs_completed.rrd (jobs)
#	slurmmond_sdiag_jobs_canceled.rrd (jobs)
#	slurmmond_sdiag_jobs_failed.rrd (jobs)

#	slurmmond_sdiag_main_last_cycle.rrd (microseconds)
#	slurmmond_sdiag_main_max_cycle.rrd (microseconds)
#	slurmmond_sdiag_main_total_cycles.rrd (cycles)
#	slurmmond_sdiag_main_mean_cycle.rrd (microseconds)
#	slurmmond_sdiag_main_mean_depth_cycle.rrd (jobs)
#	slurmmond_sdiag_main_cycles_per_minute.rrd (cycles/min)
#	slurmmond_sdiag_main_last_queue_length.rrd (jobs)

#	slurmmond_sdiag_backfill_total_backfilled_jobs_since_last_slurm_start.rrd (jobs)
#	slurmmond_sdiag_backfill_total_backfilled_jobs_since_last_stats_cycle_start.rrd (jobs)
#	slurmmond_sdiag_backfill_total_cycles.rrd (cycles)
#	slurmmond_sdiag_backfill_last_cycle.rrd (microseconds)
#	slurmmond_sdiag_backfill_max_cycle.rrd (microseconds)
#	slurmmond_sdiag_backfill_mean_cycle.rrd (microseconds)
#	slurmmond_sdiag_backfill_last_depth_cycle.rrd (jobs)
#	slurmmond_sdiag_backfill_last_depth_cycle_try_sched.rrd (jobs)
#	slurmmond_sdiag_backfill_depth_mean.rrd (jobs)
#	slurmmond_sdiag_backfill_depth_mean_try_depth.rrd (jobs)
#	slurmmond_sdiag_backfill_last_queue_length.rrd (jobs)
#	slurmmond_sdiag_backfill_queue_length_mean.rrd (jobs)


	#--- EXPIRE (missing sched type)

	slurmmond_sdiag_max_cycle.rrd
	slurmmond_sdiag_mean_cycle.rrd
	slurmmond_sdiag_mean_depth_cycle.rrd
	slurmmond_sdiag_queue_length_mean.rrd
	slurmmond_sdiag_total_backfilled_jobs_since_last_slurm_start.rrd
	slurmmond_sdiag_total_backfilled_jobs_since_last_stats_cycle_start.rrd
	slurmmond_sdiag_total_cycles.rrd
	slurmmond_sdiag_cycles_per_minute.rrd
	slurmmond_sdiag_depth_mean.rrd
	slurmmond_sdiag_depth_mean_try_depth.rrd
	slurmmond_sdiag_last_cycle.rrd
	slurmmond_sdiag_last_depth_cycle.rrd
	slurmmond_sdiag_last_depth_cycle_try_sched.rrd
	slurmmond_sdiag_last_queue_length.rrd
"""

import sys, os, traceback, time, re, getopt, syslog, signal, subprocess, multiprocessing

daemon = False
verbose = False
debug = False
pretend = False
log_to_stderr = False
fake_probejob = False

try:
	opts, args = getopt.gnu_getopt(sys.argv[1:], 'vdph', ('daemon', 'verbose', 'debug', 'pretend', 'log-to-stderr', 'help',))
except getopt.GetoptError, e:
	sys.stderr.write("*** ERROR **** unable to process command line options: %s\n" % e)
	sys.exit(1)
for opt, optarg in opts:
	if opt in ('--daemon',):
		daemon = True
	
	elif opt in ('-v', '--verbose'):
		verbose = True
	elif opt in ('-d', '--debug'):
		debug = True
	elif opt in ('-p', '--pretend'):
		pretend = True
	elif opt in ('--log-to-stderr',):
		log_to_stderr = True

	elif opt in ('-h', '--help'):
		sys.stdout.write(__doc__)
		sys.exit(0)

progname = os.path.basename(__file__)
logoption = syslog.LOG_PID
if log_to_stderr:
	logoption |= syslog.LOG_PERROR



#===



#--- config

#though many of these have "default" in the name, there is no way of changing them

metric_prefix = '%s_' % progname

#probejob sbatch parameters
probejob_default_J = 'probejob'
probejob_default_n = 1
probejob_default_t = 2
probejob_default_mem = 10
probejob_default_o = '/dev/null'
probejob_default_e = '/dev/null'
probejob_default_command = 'true'
probejob_default_priority = 999999999

#metric_interval_*
#the time to sleep between metric collection runs
metric_interval_sdiag = 120
metric_interval_jobcount = 120
metric_interval_probejob = 120

#metric_fail_retry_interval:
#if a metric run fails, how long to wait before submitting again, in seconds
metric_fail_retry_interval = 120

#probejob_query_interval:
#the time to wait between pokes to slurm to see if the probe job is no longer pending
probejob_query_interval = 10




#--- helpers

def log(msg):
	syslog.syslog(msg)

def shQuote(text):
	"""quote the given text so that it is a single, safe string in sh code.

	Note that this leaves literal newlines alone (sh and bash are fine with that, but other tools may mess them up and need to do some special handling on the output of this function).
	"""
	return "'%s'" % text.replace("'", r"'\''")

def daemonize(chdir='/'):
	"""Daemonize using the standard setsid/fork method.
	
	The continues execution in a daemon process, i.e. a process that:
	1) is adopted by init
	2) has no controlling terminal
	3) is in a session with no session leader
	4) is in a process group with no process group leader
	5) has no files from the calling environment left open

	This only redirects i/o to/from the given stdin/stdout/stderr if those respecitive streams are ttys, otherwise it leaves them alone.
	"""
	
	#just to be safe, so buffered output is not double written by both parent and child
	#note that parents are already using os._exit() below, which doesn't flush buffers, instead of sys.exit()
	for f in sys.stdout, sys.stderr: f.flush()
	
	#if this is a process group leader it cannot call setsid; fork and continue in the child (which won't be a process group leader)
	if os.getpid()==os.getpgid(0):
		pid = os.fork()
		if pid>0: os._exit(0)
	
	#get rid of the controlling terminal and put this in a new session and process group
	os.setsid()
	
	#fork, to make sure no parent hangs waiting (possibly already taken care of by the fork above), and to make the new session and new process group have no leaders
	pid = os.fork()
	if pid>0: os._exit(0)
	
	#don't leave open anything from the calling environment
	os.chdir(chdir)
	#os.umask(0)

	if sys.stdin.isatty():
		si = file('/dev/null', 'r')
		os.dup2(si.fileno(), sys.stdin.fileno())
	if sys.stdout.isatty():
		so = file('/dev/null', 'a+')
		os.dup2(so.fileno(), sys.stdout.fileno())
	if sys.stderr.isatty():
		se = file('/dev/null', 'a+', 0)
		os.dup2(se.fileno(), sys.stderr.fileno())



#--- misc

re_nonalphanum = re.compile(r'[^a-zA-Z0-9]+')



#--- metrics

#these yield tuples representing metrics, and they do not loop

def metrics_sdiag():
	global progname

	sh = 'sdiag'
	p = subprocess.Popen(sh, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
	stdout, stderr = [ x.strip() for x in p.communicate()]
	if p.returncode!=0 or stderr!='':
		raise Exception("sdiag failed with non-zero returncode [%d] and/or non-empty stderr [%r]" % (sh, p.returncode, stderr))

	##e.g.
	#*******************************************************
	#sdiag output at Mon Dec  9 16:45:13 2013
	#Data since      Mon Dec  9 15:37:51 2013
	#*******************************************************
	#Server thread count: 3
	#Agent queue size:    0
	#
	#Jobs submitted: 8
	#Jobs started:   0
	#Jobs completed: 0
	#Jobs canceled:  0
	#Jobs failed:    0
	#
	#Main schedule statistics (microseconds):
	#		Last cycle:   1138
	#		Max cycle:    2951
	#		Total cycles: 71
	#		Mean cycle:   1192
	#		Mean depth cycle:  184
	#		Cycles per minute: 1
	#		Last queue length: 190
	#
	#Backfilling stats
	#		Total backfilled jobs (since last slurm start): 0
	#		Total backfilled jobs (since last stats cycle start): 0
	#		Total cycles: 6
	#		Last cycle when: Mon Dec  9 15:43:29 2013
	#		Last cycle: 6052
	#		Max cycle:  6287
	#		Mean cycle: 5922
	#		Last depth cycle: 190
	#		Last depth cycle (try sched): 0
	#		Depth Mean: 185
	#		Depth Mean (try depth): 0
	#		Last queue length: 190
	#		Queue length mean: 185
	
	sched = None  #main or backfill
	
	for line in stdout.strip().split('\n'):
		line = line.strip()

		if line.startswith('Main schedule statistics'):
			sched = 'main'
			continue
		if line.startswith('Backfilling stats'):  #it may also say "WARNING: data obtained in the middle of backfilling execution", but I'm not doing anything different in that case yet
			sched = 'backfill'
			continue

		try:
			name, value = line.split(':')
			value = int(value)

			name = re_nonalphanum.sub('_', name).strip('_').lower()

			units = '(units unknown)'
			if 'jobs' in name:
				units = 'jobs'
			elif name.endswith('depth_cycle') or \
				 name.endswith('depth_cycle_try_sched') or \
				 name.endswith('depth_mean') or \
				 name.endswith('depth_mean_try_depth'):
				units = 'jobs'
			elif name.endswith('cycle'):  #(mean_depth_cycle already handled above)
				units = 'microseconds'
			elif name.endswith('cycles'):
				units = 'cycles'
			elif 'cycles_per_minute' in name:
				units = 'cycles/min'
			elif 'queue_length' in name:
				units = 'jobs'
			elif name.endswith('thread_count'):
				units = 'threads'
			elif name.endswith('agent_queue_size'):
				units = 'agents'
			
			if sched is not None:
				name = '%s_%s' % (sched, name)
			name = '%ssdiag_%s' % (metric_prefix, name)
			
			#(NAME, VALUE, TYPE, UNITS, GROUP, TITLE, DESCRIPTION)
			yield (name, value, 'int32', units, progname, name, name)
		except ValueError:
			#e.g. dates
			pass

def metrics_jobcount():
	global progname

	for name, sh in (
		('%sjobcount_total_pending'        % metric_prefix, "squeue -h -o '%u' -t PD | wc -l"),
		('%sjobcount_total_running'        % metric_prefix, "squeue -h -o '%u' -t R  | wc -l"),
		('%sjobcount_max_pending_one_user' % metric_prefix, "squeue -h -o '%u' -t PD | sort | uniq -c | sort -n | tail -n 1 | awk '{print $1}'"),
		('%sjobcount_max_running_one_user' % metric_prefix, "squeue -h -o '%u' -t R  | sort | uniq -c | sort -n | tail -n 1 | awk '{print $1}'"),
	):
		p = subprocess.Popen(sh, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
		stdout, stderr = [ x.strip() for x in p.communicate()]
		if p.returncode!=0 or stderr!='':
			raise Exception("squeue command [%s] failed with non-zero returncode [%d] and/or non-empty stderr [%r]" % (sh, p.returncode, stderr))

		if stdout=='': stdout = '0'
		
		#(NAME, VALUE, TYPE, UNITS, GROUP, TITLE, DESCRIPTION)
		yield (name, stdout, 'uint32', 'jobs', progname, name, name)

def metrics_probejob(
	partition,
	J='probejob',
	n=probejob_default_n,
	t=probejob_default_t,
	mem=probejob_default_mem,
	o=probejob_default_o,
	e=probejob_default_e,
	command=probejob_default_command,
	priority=probejob_default_priority,
	):

	global progname


	#--- internal state

	t_submit = None  #unix time at submission, float
	s_pend = None    #number of seconds the job spent pending, int


	#--- build job definition
	optargs = (
		('-p', partition),
		('-J', J),
		('-n', n),
		('-t', t),
		('--mem', mem),
		('-o', o),
		('-e', e),
	)
	
	sh = 'sbatch'
	for x in optargs:
		sh += ' %s %s' % (shQuote(x[0]), shQuote(str(x[1])))
	sh += ' --wrap %s' % shQuote(command)

	
	#--- submit job
	
	if debug:
		log("DEBUG: submitting probejob with [%r]" % sh)
	#(how long this takes is not of interest, as it won't take longer than the short MessageTimeout, and hitting that causes an exception below)
	
	if fake_probejob:
		stdout, stderr = 'Submitted batch job 1234', ''
	else:
		p = subprocess.Popen(sh, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
		stdout, stderr = [ x.strip() for x in p.communicate()]
		if p.returncode!=0 or stderr!='':
			raise Exception("job submission [%r] failed with non-zero returncode [%d] and/or non-empty stderr [%r]" % (sh, p.returncode, stderr))
	
	#note the time we submitted so that we can compute how long it ends up pending
	t_submit = time.time()

	#extract JOBID from "Submitted batch job JOBID"
	try:
		jobid = int(stdout.rsplit(' ',1)[1])
	except (ValueError, IndexError):
		raise Exception("unable to parse jobid from [%r]" % stdout)
	
	if debug:
		log("submitted probejob, assigned jobid [%d]" % jobid)
	

	#--- update its priority

	sh = "sudo -u slurm scontrol update JobId=%s Priority=%s" % (shQuote(str(jobid)), shQuote(str(priority)))
	if debug:
		log("DEBUG: updating probejob priority with [%r]" % sh)
	
	if fake_probejob:
		stdout, stderr = '', ''
	else:
		p = subprocess.Popen(sh, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
		stdout, stderr = [ x.strip() for x in p.communicate()]
		if p.returncode!=0 or stderr!='':
			raise Exception("job priority update [%r] failed with non-zero returncode [%d] and/or non-empty stderr [%r]" % (sh, p.returncode, stderr))


	#--- wait for job not be pending and then wait for job to finish

	#I wish I had a way to actually wait on the job instead of having to poke slurm in a loop
	#"scontrol wait_job JOBID" doesn't work, it just says "scontrol: Job JOBID no longer running", even while PENDING
	#(haven't decided if squeue or sacct is better here)
	sh = "squeue -h -o '%T' -t ALL -j " + shQuote(str(jobid))

	#progress:
	#	0 ~ submitted
	#	1 ~ no longer pending, but not yet done
	#	2 ~ done (success or failure, it doesn't matter)
	progress = 0

	while True:
		if debug:
			log("DEBUG: querying probejob state with [%r]" % sh)

		if fake_probejob:
			if (time.time() - t_submit) < 4:
				stdout, stderr = 'PENDING', ''
			else:
				#stdout, stderr = '', 'Invalid job id specified'
				stdout, stderr = 'COMPLETED', ''
		else:
			p = subprocess.Popen(sh, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
			stdout, stderr = [ x.strip() for x in p.communicate()]

		#interpret the job's state
		if 'Invalid job id specified' in stderr:
			progress = 2
		elif stderr=='':
			if stdout in ('COMPLETED', 'FAILED', 'CANCELED', 'TIMEOUT', 'NODEFAIL'):
				progress = 2
			elif stdout!='PENDING':
				progress = 1
		else:
			if debug:
				log("DEBUG: WARNING: unable to probejob query result stdout [%r] stderr [%r] returncode [%d]" % (jobid, stdout, stderr, p.returncode))
		
		#update the pend time if necessary
		if progress>0 and s_pend is None:
			s_pend = int(round(time.time() - t_submit))
			if debug:
				log("probejob [%d] no longer pending, after [%s] seconds" % (jobid, s_pend))
		
		#break or continue
		if progress==2:
			if debug:
				state = stdout
				if state=='': state = 'n/a'
				log("probejob [%d] done, last seen in state [%s]" % (jobid, (stdout if stdout!='' else 'n/a')))
			break
		else:
			if s_pend is None:
				if debug:
					log("DEBUG: probejob [%d] still pending" % (jobid))

			if debug:
				log("DEBUG: sleeping for [%d] seconds" % probejob_query_interval)
			time.sleep(probejob_query_interval)


	#--- report

	#(NAME, VALUE, TYPE, UNITS, GROUP, TITLE, DESCRIPTION)
	s = 'pend time for a job in %s' % partition
	yield ('%sprobejob_pendtime_%s' % (metric_prefix, partition), s_pend, 'uint32', 'seconds', progname, s, s)



#--- child processes

def child(name, metricq, metricf, loopsleep):
	"""run a child process that produces metrics
	name -- an identifying string, used in the log
	metricq -- a multiprocessing.Queue in which to put metrics
	metricf -- a function that yields metrics
	loopsleep -- interval to sleep betwen runs
	"""

	global progname
	global logoption
	name = '%s(%s)' % (progname, name)
	syslog.closelog()
	syslog.openlog(name, logoption)

	try:
		while True:
			try:
				for metric in metricf():
					metricq.put(metric)
			except Exception, e:
				log("metrics for [%s] failed with message [%s]" % (name, e))
				if debug:
					for line in ''.join(traceback.format_exception(*sys.exc_info())).strip().split('\n'):
						log("DEBUG: %s" % line)
				
				if debug:
					log("DEBUG: sleeping for [%d] seconds" % metric_fail_retry_interval)
				time.sleep(metric_fail_retry_interval)

			if debug:
				log("DEBUG: sleeping for [%d] seconds" % loopsleep)
			time.sleep(loopsleep)
			
	except (KeyboardInterrupt, SystemExit):
		#FIXME look for child processes and try to kill them
		log("shutting down")

	syslog.closelog()



#---main

if __name__=='__main__':
	#--setup

	if daemon:
		daemonize()

	syslog.openlog(progname, logoption)

	metricq = multiprocessing.Queue()

	children = []


	#--- startup

	log("starting")
	
	#sdiag metrics
	p = multiprocessing.Process(target=lambda:child('sdiag', metricq, lambda:metrics_sdiag(), metric_interval_sdiag))
	p.start()
	log("started sdiag metrics process, pid [%d]" % p.pid)
	children.append(p)
	
	#jobcount metrics
	p = multiprocessing.Process(target=lambda:child('jobcount', metricq, lambda:metrics_jobcount(), metric_interval_jobcount))
	p.start()
	log("started jobcount metrics process, pid [%d]" % p.pid)
	children.append(p)

	#probejob metrics
	for partition in ('interact', 'general'):
		name = 'probejob-%s' % partition
		p = multiprocessing.Process(target=lambda:child(name, metricq, lambda:metrics_probejob(partition), metric_interval_probejob))
		p.start()
		log("started [%s] metrics process, pid [%d]" % (name, p.pid))
		children.append(p)


	#--- define shutdown

	def shutdown(signum, frame):
		log("caught signal [%d], shutting down" % signum)
		for p in children:
			if debug:
				log("DEBUG: attempting to kill child pid [%d] if necessary" % p.pid)
			try:
				p.terminate()
			except Exception:
				pass
			p.join()
		log("done")
		syslog.closelog()
		sys.exit(0)
	for s in (signal.SIGTERM, signal.SIGINT, signal.SIGHUP, signal.SIGQUIT):
		signal.signal(s, shutdown)


	#--- run
	
	while True:
		m = metricq.get()
		
		#(NAME, VALUE, TYPE, UNITS, GROUP, TITLE, DESCRIPTION)
		if debug:
			msg = "DEBUG: "
			if pretend:
				msg += "PRETEND: "
			msg += "sending metric [%s]" % str(m)
			log(msg)
		sh = 'gmetric --name %s --value %s --type %s --units %s --group %s --title %s --desc %s --tmax 86400' % (  #FIXME proper --tmax
			shQuote(str(m[0])),
			shQuote(str(m[1])),
			shQuote(str(m[2])),
			shQuote(str(m[3])),
			shQuote(str(m[4])),
			shQuote(str(m[5])),
			shQuote(str(m[6])),
		)
		if verbose:
			print sh
		if not pretend:
			p = subprocess.Popen(sh, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
			stdout, stderr = [ x.strip() for x in p.communicate()]
			if p.returncode!=0 or stderr!='':
				log("WARNING: gmetric [%s] failed with non-zero returncode [%d] and/or non-empty stderr [%r]" % (sh, p.returncode, stderr))
