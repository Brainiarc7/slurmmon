#!/usr/bin/env python

# Copyright (c) 2013-2014
# Harvard FAS Research Computing
# All rights reserved.

"""\
NAME
	slurmmond - gather data about SLURM

SYNOPSIS
	slurmmond

DESCRIPTION
	This runs several child processes that loop reporting back to the parent 
	various metrics.  The master process sends these to ganglia by running
	gmetric.

	Note that this uses python multiprocessing, so it's normal to see several 
	instances of the process in the process table.

OPTIONS
	--metrics CATEGORY
		Run the given CATEGORY of metrics.  This option may be used multiple 
		times; all given categories will be run.  Choices are:

		To be run in one place, e.g. on primary slurmctrld:

		* sdiag
		* jobcount
		* reservations
		* probejob

		To be run on each compute node:

		* computenode

	--daemon
		Run as a daemon instead of in the foreground as a child of the calling 
		process.

	--conf CONFIGURATION_FILE
		Use the given configuration file instead of /etc/slurmmon.conf.

	-v, --verbose
		Write metrics to stdout, too (as gmetric commands).

	-d, --debug
		Log extra debugging information.  Even without --daemon, you still need 
		to look in syslog to find them.

	-p, --pretend
		Don't actually send gmetrics.

	--log-to-stderr
		Log messages to stderr in addition to syslog.

	-h, --help
		Print this help.

EXAMPLES
	To debug a CATEGORY without actually sending any metrics, run the following 
	and watch syslog:

		$ ./usr/sbin/slurmmond -v -d -p --conf ./etc/slurmmon.conf --metrics CATEGORY

REQUIREMENTS
	n/a

BUGS/TODO
	n/a

AUTHOR
	Copyright (c) 2013
	Harvard FAS Research Computing
"""

"""
architecture
	metrics for individual categories are run as parallel sub-processes (using multiprocessing)
	they yield metrics -- tuples in the form (NAME, VALUE, TYPE, UNITS, GROUP, TITLE, DESCRIPTION)

metrics (subject to change)
	#--jobcount

	slurmmond_jobcount_total_pending.rrd (jobs)
	slurmmond_jobcount_total_running.rrd (jobs)
	slurmmond_jobcount_max_pending_one_user.rrd (jobs)
	slurmmond_jobcount_max_running_one_user.rrd (jobs)


	#--- probejob

	slurmmond_probejob_pendtime_PARTITION.rrd (seconds), for each PARTITION


	#--- sdiag

	slurmmond_sdiag_server_thread_count.rrd (threads)
	slurmmond_sdiag_agent_queue_size.rrd (agents)

	slurmmond_sdiag_jobs_submitted.rrd (jobs)
	slurmmond_sdiag_jobs_started.rrd (jobs)
	slurmmond_sdiag_jobs_completed.rrd (jobs)
	slurmmond_sdiag_jobs_canceled.rrd (jobs)
	slurmmond_sdiag_jobs_failed.rrd (jobs)

	slurmmond_sdiag_main_last_cycle.rrd (microseconds)
	slurmmond_sdiag_main_max_cycle.rrd (microseconds)
	slurmmond_sdiag_main_total_cycles.rrd (cycles)
	slurmmond_sdiag_main_mean_cycle.rrd (microseconds)
	slurmmond_sdiag_main_mean_depth_cycle.rrd (jobs)
	slurmmond_sdiag_main_cycles_per_minute.rrd (cycles/min)
	slurmmond_sdiag_main_last_queue_length.rrd (jobs)

	slurmmond_sdiag_backfill_total_backfilled_jobs_since_last_slurm_start.rrd (jobs)
	slurmmond_sdiag_backfill_total_backfilled_jobs_since_last_stats_cycle_start.rrd (jobs)
	slurmmond_sdiag_backfill_total_cycles.rrd (cycles)
	slurmmond_sdiag_backfill_last_cycle.rrd (microseconds)
	slurmmond_sdiag_backfill_max_cycle.rrd (microseconds)
	slurmmond_sdiag_backfill_mean_cycle.rrd (microseconds)
	slurmmond_sdiag_backfill_last_depth_cycle.rrd (jobs)
	slurmmond_sdiag_backfill_last_depth_cycle_try_sched.rrd (jobs)
	slurmmond_sdiag_backfill_depth_mean.rrd (jobs)
	slurmmond_sdiag_backfill_depth_mean_try_depth.rrd (jobs)
	slurmmond_sdiag_backfill_last_queue_length.rrd (jobs)
	slurmmond_sdiag_backfill_queue_length_mean.rrd (jobs)


	#--- reservations

	slurmmond_cores_in_reservations (CPU cores)
"""


import sys, os, traceback, time, re, getopt, syslog, socket, signal, json, subprocess, multiprocessing
try:
	import slurmmon
except ImportError:
	#try adding where it exists if this is being run out of the source tree
	sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..', 'lib/python/site-packages')))
	import slurmmon
from slurmmon import util, nodes


#default settings
metrics = []
daemon = False
conf = '/etc/slurmmon.conf'
verbose = False
debug = False
pretend = False
log_to_stderr = False
fake_probejob = False

try:
	opts, args = getopt.gnu_getopt(sys.argv[1:], 'vdph', ('metrics=', 'daemon', 'conf=', 'verbose', 'debug', 'pretend', 'log-to-stderr', 'help',))
except getopt.GetoptError, e:
	sys.stderr.write('*** ERROR **** unable to process command line options: %s\n' % e)
	sys.exit(1)
for opt, optarg in opts:
	if opt in ('--metrics',):
		metrics.append(optarg)
	if opt in ('--daemon',):
		daemon = True
	if opt in ('--conf',):
		conf = optarg

	elif opt in ('-v', '--verbose'):
		verbose = True
	elif opt in ('-d', '--debug'):
		debug = True
	elif opt in ('-p', '--pretend'):
		pretend = True
	elif opt in ('--log-to-stderr',):
		log_to_stderr = True

	elif opt in ('-h', '--help'):
		sys.stdout.write(__doc__)
		sys.exit(0)

PROGNAME = os.path.basename(__file__)
LOGOPTIONS = syslog.LOG_PID
if log_to_stderr:
	LOGOPTIONS |= syslog.LOG_PERROR

if len(metrics)==0:
	sys.stderr.write("*** ERROR *** no --metrics specified\n")
	sys.exit(1)



#===



#--- config

conf = json.load(open(conf, 'r'))

#basic stuff
metric_prefix = conf.get('metric_prefix', '%s_' % PROGNAME)

#partitions to which to send probe jobs
probejob_partitions = [ x.encode('utf-8') for x in conf.get('probejob_partitions', ())]

#probejob sbatch parameters
probejob_default_J        = conf.get('probejob_default_J', 'probejob').encode('utf-8')
probejob_default_n        = conf.get('probejob_default_n', 1)
probejob_default_t        = conf.get('probejob_default_t', 2)
probejob_default_mem      = conf.get('probejob_default_mem', 10)
probejob_default_o        = conf.get('probejob_default_o', '/dev/null').encode('utf-8')
probejob_default_e        = conf.get('probejob_default_e', '/dev/null').encode('utf-8')
probejob_default_command  = conf.get('probejob_default_command' , 'true').encode('utf-8')
probejob_default_priority = conf.get('probejob_default_priority', 999999999)

#metric_interval_*
#the time to sleep between metric collection runs
metric_interval_sdiag        = conf.get('metric_interval_sdiag'   ,     120)
metric_interval_jobcount     = conf.get('metric_interval_jobcount',     120)
metric_interval_reservations = conf.get('metric_interval_reservations', 120)
metric_interval_probejob     = conf.get('metric_interval_probejob',     120)
metric_interval_computenode  = conf.get('metric_interval_computenode',  120)

#metric_fail_retry_interval:
#if a metric run fails, how long to wait before submitting again, in seconds
metric_fail_retry_interval = conf.get('metric_fail_retry_interval', 120)

#probejob_query_interval:
#the time to wait between pokes to slurm to see if the probe job is no longer pending
probejob_query_interval = conf.get('probejob_query_interval', 10)



#--- helpers

def log(msg):
	syslog.syslog(msg)

def daemonize(chdir='/'):
	"""Daemonize using the standard setsid/fork method.

	The continues execution in a daemon process, i.e. a process that:
	1) is adopted by init
	2) has no controlling terminal
	3) is in a session with no session leader
	4) is in a process group with no process group leader
	5) has no files from the calling environment left open

	This only redirects i/o to/from the given stdin/stdout/stderr if those respective streams are ttys, otherwise it leaves them alone.
	"""

	#just to be safe, so buffered output is not double written by both parent and child
	#note that parents are already using os._exit() below, which doesn't flush buffers, instead of sys.exit()
	for f in sys.stdout, sys.stderr: f.flush()

	#if this is a process group leader it cannot call setsid; fork and continue in the child (which won't be a process group leader)
	if os.getpid()==os.getpgid(0):
		pid = os.fork()
		if pid>0: os._exit(0)

	#get rid of the controlling terminal and put this in a new session and process group
	os.setsid()

	#fork, to make sure no parent hangs waiting (possibly already taken care of by the fork above), and to make the new session and new process group have no leaders
	pid = os.fork()
	if pid>0: os._exit(0)

	#don't leave open anything from the calling environment
	os.chdir(chdir)
	#os.umask(0)

	if sys.stdin.isatty():
		si = file('/dev/null', 'r')
		os.dup2(si.fileno(), sys.stdin.fileno())
	if sys.stdout.isatty():
		so = file('/dev/null', 'a+')
		os.dup2(so.fileno(), sys.stdout.fileno())
	if sys.stderr.isatty():
		se = file('/dev/null', 'a+', 0)
		os.dup2(se.fileno(), sys.stderr.fileno())



#--- misc

re_nonalphanum = re.compile(r'[^a-zA-Z0-9]+')



#--- metrics

#these yield tuples representing metrics, and they do not loop

def metrics_sdiag():
	stdout = util.runsh(['sdiag'])
	##e.g.
	#*******************************************************
	#sdiag output at Mon Dec  9 16:45:13 2013
	#Data since      Mon Dec  9 15:37:51 2013
	#*******************************************************
	#Server thread count: 3
	#Agent queue size:    0
	#
	#Jobs submitted: 8
	#Jobs started:   0
	#Jobs completed: 0
	#Jobs canceled:  0
	#Jobs failed:    0
	#
	#Main schedule statistics (microseconds):
	#		Last cycle:   1138
	#		Max cycle:    2951
	#		Total cycles: 71
	#		Mean cycle:   1192
	#		Mean depth cycle:  184
	#		Cycles per minute: 1
	#		Last queue length: 190
	#
	#Backfilling stats
	#		Total backfilled jobs (since last slurm start): 0
	#		Total backfilled jobs (since last stats cycle start): 0
	#		Total cycles: 6
	#		Last cycle when: Mon Dec  9 15:43:29 2013
	#		Last cycle: 6052
	#		Max cycle:  6287
	#		Mean cycle: 5922
	#		Last depth cycle: 190
	#		Last depth cycle (try sched): 0
	#		Depth Mean: 185
	#		Depth Mean (try depth): 0
	#		Last queue length: 190
	#		Queue length mean: 185

	sched = None  #main or backfill

	for line in stdout.strip().split('\n'):
		line = line.strip()

		if line.startswith('Main schedule statistics'):
			sched = 'main'
			continue
		if line.startswith('Backfilling stats'):  #it may also say "WARNING: data obtained in the middle of backfilling execution", but I'm not doing anything different in that case yet
			sched = 'backfill'
			continue

		try:
			name, value = line.split(':')
			value = int(value)

			name = re_nonalphanum.sub('_', name).strip('_').lower()

			units = '(units unknown)'
			if 'jobs' in name:
				units = 'jobs'
			elif name.endswith('depth_cycle') or \
				 name.endswith('depth_cycle_try_sched') or \
				 name.endswith('depth_mean') or \
				 name.endswith('depth_mean_try_depth'):
				units = 'jobs'
			elif name.endswith('cycle'):  #(mean_depth_cycle already handled above)
				units = 'microseconds'
			elif name.endswith('cycles'):
				units = 'cycles'
			elif 'cycles_per_minute' in name:
				units = 'cycles/min'
			elif 'queue_length' in name:
				units = 'jobs'
			elif name.endswith('thread_count'):
				units = 'threads'
			elif name.endswith('agent_queue_size'):
				units = 'agents'

			if sched is not None:
				name = '%s_%s' % (sched, name)
			name = '%ssdiag_%s' % (metric_prefix, name)

			#(NAME, VALUE, TYPE, UNITS, GROUP, TITLE, DESCRIPTION)
			yield (name, value, 'int32', units, PROGNAME, name, name)
		except ValueError:
			#e.g. dates
			pass

def metrics_jobcount():
	for name, sh in (
		('%sjobcount_total_pending'        % metric_prefix, "squeue -h -o '%u' -t PD | wc -l"),
		('%sjobcount_total_running'        % metric_prefix, "squeue -h -o '%u' -t R  | wc -l"),
		('%sjobcount_max_pending_one_user' % metric_prefix, "squeue -h -o '%u' -t PD | sort | uniq -c | sort -n | tail -n 1 | awk '{print $1}'"),
		('%sjobcount_max_running_one_user' % metric_prefix, "squeue -h -o '%u' -t R  | sort | uniq -c | sort -n | tail -n 1 | awk '{print $1}'"),
	):
		stdout = util.runsh(sh).strip()
		if stdout == '':
			stdout = '0'

		#(NAME, VALUE, TYPE, UNITS, GROUP, TITLE, DESCRIPTION)
		yield (name, stdout, 'uint32', 'jobs', PROGNAME, name, name)

def metrics_reservations():
	stdout = util.runsh(['scontrol', 'show', 'reservations', '--oneliner']).strip()

	reserved_cores = 0
	if 'No reservations in the system' in stdout:
		# Fall through and return 0 reserved cores.
		pass
	else:
		for line in stdout.split('\n'):
			reservation = {}
			for item in re.split(r'\s+', line):
				field, value = item.split('=')
				reservation[field] = value

			reserved_cores += int(reservation['CoreCnt'])

	name = '%scores_in_reservations' % metric_prefix
	yield (name, reserved_cores, 'int32', 'CPU cores',
		PROGNAME, name, name)

def metrics_probejob(
	partition,
	J='probejob',
	n=probejob_default_n,
	t=probejob_default_t,
	mem=probejob_default_mem,
	o=probejob_default_o,
	e=probejob_default_e,
	command=probejob_default_command,
	priority=probejob_default_priority,
	):

	#--- internal state

	t_submit = None  #unix time at submission, float
	s_pend = None    #number of seconds the job spent pending, int


	#--- build job definition
	optargs = (
		('-p', partition),
		('-J', J),
		('-n', n),
		('-t', t),
		('--mem', mem),
		('-o', o),
		('-e', e),
	)

	sh = 'sbatch'
	for x in optargs:
		sh += ' %s %s' % (util.shquote(x[0]), util.shquote(str(x[1])))
	sh += ' --wrap %s' % util.shquote(command)


	#--- submit job

	if debug:
		log('DEBUG: submitting probejob with [%r]' % sh)
	#(how long this takes is not of interest, as it won't take longer than the short MessageTimeout, and hitting that causes an exception below)

	if fake_probejob:
		stdout, stderr = 'Submitted batch job 1234', ''
	else:
		p = subprocess.Popen(sh, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
		stdout, stderr = [ x.strip() for x in p.communicate()]
		if p.returncode!=0 or stderr!='':
			raise Exception('job submission [%r] failed with non-zero returncode [%d] and/or non-empty stderr [%r]' % (sh, p.returncode, stderr))

	#note the time we submitted so that we can compute how long it ends up pending
	t_submit = time.time()

	#extract JOBID from "Submitted batch job JOBID"
	try:
		jobid = int(stdout.rsplit(' ',1)[1])
	except (ValueError, IndexError):
		raise Exception('unable to parse jobid from [%r]' % stdout)

	if debug:
		log('submitted probejob, assigned jobid [%d]' % jobid)


	#--- update its priority

	sh = 'sudo -u slurm scontrol update JobId=%s Priority=%s' % (util.shquote(str(jobid)), util.shquote(str(priority)))
	if debug:
		log('DEBUG: updating probejob priority with [%r]' % sh)

	if fake_probejob:
		stdout, stderr = '', ''
	else:
		p = subprocess.Popen(sh, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
		stdout, stderr = [ x.strip() for x in p.communicate()]
		if p.returncode!=0 or stderr!='':
			raise Exception('job priority update [%r] failed with non-zero returncode [%d] and/or non-empty stderr [%r]' % (sh, p.returncode, stderr))


	#--- wait for job not be pending and then wait for job to finish

	#I wish I had a way to actually wait on the job instead of having to poke slurm in a loop
	#"scontrol wait_job JOBID" doesn't work, it just says "scontrol: Job JOBID no longer running", even while PENDING
	#(haven't decided if squeue or sacct is better here)
	sh = "squeue -h -o '%T' -t ALL -j " + util.shquote(str(jobid))

	#progress:
	#	0 ~ submitted
	#	1 ~ no longer pending, but not yet done
	#	2 ~ done (success or failure, it doesn't matter)
	progress = 0

	while True:
		if debug:
			log('DEBUG: querying probejob state with [%r]' % sh)

		if fake_probejob:
			if (time.time() - t_submit) < 4:
				stdout, stderr = 'PENDING', ''
			else:
				#stdout, stderr = '', 'Invalid job id specified'
				stdout, stderr = 'COMPLETED', ''
		else:
			p = subprocess.Popen(sh, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
			stdout, stderr = [ x.strip() for x in p.communicate()]

		#interpret the job's state
		if 'Invalid job id specified' in stderr:
			progress = 2
		elif stderr=='':
			if stdout in ('COMPLETED', 'FAILED', 'CANCELED', 'TIMEOUT', 'NODEFAIL'):
				progress = 2
			elif stdout!='PENDING':
				progress = 1
		else:
			if debug:
				log('DEBUG: WARNING: unable to probejob query result stdout [%r] stderr [%r] returncode [%d]' % (jobid, stdout, stderr, p.returncode))

		#update the pend time if necessary
		if progress>0 and s_pend is None:
			s_pend = int(round(time.time() - t_submit))
			if debug:
				log('probejob [%d] no longer pending, after [%s] seconds' % (jobid, s_pend))

		#break or continue
		if progress==2:
			if debug:
				state = stdout
				if state=='': state = 'n/a'
				log('probejob [%d] done, last seen in state [%s]' % (jobid, (stdout if stdout!='' else 'n/a')))
			break
		else:
			if s_pend is None:
				if debug:
					log('DEBUG: probejob [%d] still pending' % (jobid))

			if debug:
				log('DEBUG: sleeping for [%d] seconds' % probejob_query_interval)
			time.sleep(probejob_query_interval)


	#--- report

	#(NAME, VALUE, TYPE, UNITS, GROUP, TITLE, DESCRIPTION)
	s = 'pend time for a job in %s' % partition
	yield ('%sprobejob_pendtime_%s' % (metric_prefix, partition), s_pend, 'uint32', 'seconds', PROGNAME, s, s)

def metrics_computenode():
	n = nodes.Node(NodeName=util.get_hostname())

	#note that these may raise KeyError or be None
	cores_total = n['OS_Cores_Total']
	cores_used = n['OS_Cores_Used']
	memory_total = n['OS_Memory_Total']
	memory_used = n['OS_Memory_Used']
	cores_allocated = n['CPUAlloc']
	memory_allocated = n['AllocMem_kB']

	#(NAME, VALUE, TYPE, UNITS, GROUP, TITLE, DESCRIPTION)
	
	name = '%scores_total' % metric_prefix
	yield (name, cores_total, 'uint32', 'cores', PROGNAME, name, name)
	
	name = '%scores_used' % metric_prefix
	yield (name, cores_used, 'uint32', 'cores', PROGNAME, name, name)
	
	name = '%scores_allocated' % metric_prefix
	yield (name, cores_allocated, 'uint32', 'cores', PROGNAME, name, name)
	
	name = '%smemory_total' % metric_prefix
	yield (name, memory_total, 'uint32', 'kB', PROGNAME, name, name)
	
	name = '%smemory_used' % metric_prefix
	yield (name, memory_used, 'uint32', 'kB', PROGNAME, name, name)
	
	name = '%smemory_allocated' % metric_prefix
	yield (name, memory_allocated, 'uint32', 'kB', PROGNAME, name, name)
	
	name = '%spcpu_utilization' % metric_prefix
	yield (name, float(cores_used)/cores_total * 100, 'float', 'percent', PROGNAME, name, name)
	name = '%spmem_utilization' % metric_prefix
	yield (name, float(memory_used)/memory_total * 100, 'float', 'percent', PROGNAME, name, name)
	
	name = '%spcpu_allocation' % metric_prefix
	yield (name, float(cores_allocated)/cores_total * 100, 'float', 'percent', PROGNAME, name, name)
	name = '%spmem_allocation' % metric_prefix
	yield (name, float(memory_allocated)/memory_total * 100, 'float', 'percent', PROGNAME, name, name)



#--- child processes

def child(name, metricq, metricf, loopsleep):
	"""run a child process that produces metrics
	name -- an identifying string, used in the log
	metricq -- a multiprocessing.Queue in which to put metrics
	metricf -- a function that yields metrics
	loopsleep -- interval to sleep betwen runs
	"""

	name = '%s(%s)' % (PROGNAME, name)
	syslog.closelog()
	syslog.openlog(name, LOGOPTIONS)

	try:
		while True:
			try:
				for metric in metricf():
					metricq.put(metric)
			except Exception, e:
				log('metrics for [%s] failed with message [%s]' % (name, e))
				if debug:
					for line in ''.join(traceback.format_exception(*sys.exc_info())).strip().split('\n'):
						log('DEBUG: %s' % line)
					log('DEBUG: sleeping for [%d] seconds' % metric_fail_retry_interval)

				time.sleep(metric_fail_retry_interval)

			if debug:
				log('DEBUG: sleeping for [%d] seconds' % loopsleep)
			time.sleep(loopsleep)
	except (KeyboardInterrupt, SystemExit):
		#FIXME look for child processes and try to kill them
		log('shutting down')

	syslog.closelog()



#---main

if __name__=='__main__':
	#--setup

	if daemon:
		daemonize()

	syslog.openlog(PROGNAME, LOGOPTIONS)

	metricq = multiprocessing.Queue()

	children = []


	#--- startup

	log('starting')

	if 'sdiag' in metrics:
		p = multiprocessing.Process(target=lambda:child('sdiag', metricq, lambda:metrics_sdiag(), metric_interval_sdiag))
		p.start()
		log('started sdiag metrics process, pid [%d]' % p.pid)
		children.append(p)

	if 'jobcount' in metrics:
		p = multiprocessing.Process(target=lambda:child('jobcount', metricq, lambda:metrics_jobcount(), metric_interval_jobcount))
		p.start()
		log('started jobcount metrics process, pid [%d]' % p.pid)
		children.append(p)

	if 'reservations' in metrics:
		p = multiprocessing.Process(target=lambda: child('reservations',
			metricq, lambda: metrics_reservations(), metric_interval_reservations))
		p.start()
		log('started reserved cores metrics process, pid [%d]' % p.pid)
		children.append(p)

	if 'probejob' in metrics:
		for partition in probejob_partitions:
			name = 'probejob-%s' % partition
			p = multiprocessing.Process(target=lambda:child(name, metricq, lambda:metrics_probejob(partition), metric_interval_probejob))
			p.start()
			log('started [%s] metrics process, pid [%d]' % (name, p.pid))
			children.append(p)

	if 'computenode' in metrics:
		p = multiprocessing.Process(target=lambda:child('computenode', metricq, lambda:metrics_computenode(), metric_interval_computenode))
		p.start()
		log('started computenode metrics process, pid [%d]' % p.pid)
		children.append(p)


	#--- define shutdown

	def shutdown(signum, frame):
		log('caught signal [%d], shutting down' % signum)
		for p in children:
			if debug:
				log('DEBUG: attempting to kill child pid [%d] if necessary' % p.pid)
			try:
				p.terminate()
			except Exception:
				pass
			p.join()
		log('done')
		syslog.closelog()
		sys.exit(0)
	for s in (signal.SIGTERM, signal.SIGINT, signal.SIGHUP, signal.SIGQUIT):
		signal.signal(s, shutdown)


	#--- run

	while True:
		# (NAME, VALUE, TYPE, UNITS, GROUP, TITLE, DESCRIPTION)
		m = metricq.get()

		if debug:
			msg = 'DEBUG: '
			if pretend:
				msg += 'PRETEND: '
			msg += 'sending metric [%s]' % str(m)
			log(msg)
		shv = [
			'gmetric',
			'--name', str(m[0]), '--value', str(m[1]),
			'--type', str(m[2]), '--units', str(m[3]),
			'--group', str(m[4]), '--title', str(m[5]),
			'--desc', str(m[6]),
			# FIXME: proper --tmax
			'--tmax', '86400'
		]
		if verbose:
			print ' '.join(shv)
		if not pretend:
			util.runsh(shv)
